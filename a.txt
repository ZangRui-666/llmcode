using lamma as reference model to get the reference loss which is the probability that a reference model assigns to that token, given the preceding tokens and the formula is " Input a dataset to reference model and save the reference loss of all tokens to a dictionary called reference_loss_dict for use in next step. Now we use this reference model to train tinyLamma model and the process is like the followings. In the training process of tinylamma, calculate the excess loss of each token which is training loss of this token minus the reference loss of this token from reference_loss_dict. We remain the loss of token which has the top 30 percent excess loss of all tokens, and modify the loss of other tokens to zero for future training. implement my above prompt in python.

the input of reference_model and tinylama model is different, as they use different datasets. And when training tinylama model and get the loss of input, you need to use token reference loss from reference model to calculate excess loss.

We use Qwen2.5-7B as reference model to train Qwen2.5-0.5B model and the process is like the followings. Our dataset can be wikimedia text. And the definition of reference model loss is that "Reference model loss = -logP(xi|x<i)", which is the negative log-likelihood of observing the token xi, given the preceding tokens. In the training process of Qwen2.5-0.5B, calculate the excess loss of each token which is training loss of this token minus the reference loss from Qwen2.5-7B. We remain the loss of token which has the top 30 percent excess loss of all tokens, and modify the loss of other tokens to zero for future training. implement my above prompt in python.
