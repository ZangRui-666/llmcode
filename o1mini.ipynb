{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'wikitext-2-raw-v1' at C:\\Users\\raymo\\.cache\\huggingface\\datasets\\wikitext\\wikitext-2-raw-v1\\0.0.0\\b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Wed Oct 16 21:29:22 2024).\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"huggingface/llama\"  # Replace with the actual model name/path\n",
    "DATASET_NAME = \"wikipedia\"  # Wikimedia dataset\n",
    "BATCH_SIZE = 4  # Adjust based on your GPU memory\n",
    "GRADIENT_ACCUMULATION_STEPS = 8\n",
    "LEARNING_RATE = 5e-5\n",
    "EPOCHS = 3\n",
    "TOP_PERCENT = 0.3  # Top 30%\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B\")\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "# Load and preprocess dataset\n",
    "# Here, we're using a subset for demonstration. Adjust as needed.\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], return_tensors=\"pt\", truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(tokenized_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "total_steps = len(dataloader) * EPOCHS // GRADIENT_ACCUMULATION_STEPS\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=total_steps//10, num_training_steps=total_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/1090 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits.shape torch.Size([4, 512, 151936])\n",
      "tensor([[[ 8.0875, 12.1481, 14.4234,  ..., -3.8342, -3.8343, -3.8341],\n",
      "         [ 8.0875, 12.1481, 14.4234,  ..., -3.8342, -3.8343, -3.8341],\n",
      "         [ 8.0875, 12.1481, 14.4234,  ..., -3.8342, -3.8343, -3.8341],\n",
      "         ...,\n",
      "         [ 8.0875, 12.1481, 14.4234,  ..., -3.8342, -3.8343, -3.8341],\n",
      "         [ 8.0875, 12.1481, 14.4234,  ..., -3.8342, -3.8343, -3.8341],\n",
      "         [ 8.0875, 12.1481, 14.4234,  ..., -3.8342, -3.8343, -3.8341]],\n",
      "\n",
      "        [[ 5.5409,  5.7478,  5.2112,  ..., -2.8926, -2.8932, -2.8928],\n",
      "         [ 5.0321,  6.1159,  2.5386,  ..., -1.4790, -1.4815, -1.4792],\n",
      "         [ 8.1329,  8.6025,  6.5732,  ..., -4.5803, -4.5809, -4.5802],\n",
      "         ...,\n",
      "         [11.2808, 13.2091, 13.7755,  ..., -3.4959, -3.4964, -3.4958],\n",
      "         [11.5600, 13.2721, 13.9856,  ..., -3.6990, -3.6994, -3.6989],\n",
      "         [11.4768, 13.2690, 13.7617,  ..., -3.5662, -3.5667, -3.5661]],\n",
      "\n",
      "        [[ 8.0875, 12.1481, 14.4234,  ..., -3.8342, -3.8343, -3.8341],\n",
      "         [ 8.0875, 12.1481, 14.4234,  ..., -3.8342, -3.8343, -3.8341],\n",
      "         [ 8.0875, 12.1481, 14.4234,  ..., -3.8342, -3.8343, -3.8341],\n",
      "         ...,\n",
      "         [ 8.0875, 12.1481, 14.4234,  ..., -3.8342, -3.8343, -3.8341],\n",
      "         [ 8.0875, 12.1481, 14.4234,  ..., -3.8342, -3.8343, -3.8341],\n",
      "         [ 8.0875, 12.1481, 14.4234,  ..., -3.8342, -3.8343, -3.8341]],\n",
      "\n",
      "        [[ 5.9255,  6.0883,  4.6763,  ..., -2.9144, -2.9134, -2.9144],\n",
      "         [ 4.4505,  5.9762,  3.0982,  ..., -5.1850, -5.1844, -5.1849],\n",
      "         [ 3.9221,  3.8019,  2.9467,  ..., -1.2026, -1.2014, -1.2025],\n",
      "         ...,\n",
      "         [11.0810, 13.3203, 15.2132,  ..., -3.9823, -3.9823, -3.9822],\n",
      "         [10.9733, 13.7657, 15.3117,  ..., -3.9553, -3.9553, -3.9551],\n",
      "         [10.6821, 13.7323, 14.7935,  ..., -4.0857, -4.0858, -4.0856]]],\n",
      "       device='cuda:0', grad_fn=<UnsafeViewBackward0>)\n",
      "input_id.shape torch.Size([4, 512])\n",
      "per_token_loss torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/1090 [00:16<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[14.7662, 14.7662, 14.7662,  ..., 14.7662, 14.7662, 14.7662],\n",
      "        [ 4.4715, 17.7612, 14.6103,  ..., 21.2722, 20.7607, 20.7276],\n",
      "        [14.7662, 14.7662, 14.7662,  ..., 14.7662, 14.7662, 14.7662],\n",
      "        [ 9.8453,  6.8113, 23.3330,  ..., 17.4453, 16.7128, 16.5263]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "    epoch_loss = 0.0\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "        logits = outputs.logits  # Shape: (batch_size, seq_length, vocab_size)\n",
    "        print(\"logits.shape\", logits.shape)\n",
    "        print(logits)\n",
    "        print(\"input_id.shape\", input_ids.shape)\n",
    "\n",
    "        logits_flat = logits.view(-1, logits.size(-1)) # shape: (batch_size, seq_length, vocab_size) -> (batch_size * seq_len, vocab_size)\n",
    "        labels_flat = input_ids.view(-1)  # shape: (batch_size * seq_len)\n",
    "        \n",
    "        loss_fn = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "        per_token_loss = loss_fn(logits_flat, labels_flat)  # shape: (batch_size * seq_len)\n",
    "        per_token_loss = per_token_loss.view(input_ids.size()) # shape: (1, seq_len)\n",
    "        print(\"per_token_loss\", per_token_loss.shape)\n",
    "        print(per_token_loss)\n",
    "        sys.exit()\n",
    "\n",
    "\n",
    "        # Shift tokens for next-token prediction\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = input_ids[:, 1:].contiguous()\n",
    "        shift_attention = attention_mask[:, 1:].contiguous()\n",
    "        \n",
    "        pre_token_loss = loss_fn(shift_logits, )\n",
    "\n",
    "        # Compute log probabilities\n",
    "\n",
    "\n",
    "        log_probs = F.log_softmax(shift_logits, dim=-1)  # (batch_size, seq_length-1, vocab_size)\n",
    "        \n",
    "        # Gather log probabilities of the correct tokens\n",
    "        shift_labels_flat = shift_labels.view(-1)\n",
    "        log_probs_flat = log_probs.view(-1, log_probs.size(-1))\n",
    "        token_log_probs = log_probs_flat[torch.arange(shift_labels_flat.size(0)), shift_labels_flat]  # (batch_size * (seq_length-1))\n",
    "        token_log_probs = token_log_probs.view(shift_labels.size())  # (batch_size, seq_length-1)\n",
    "        \n",
    "        # Compute negative log-likelihood loss per token\n",
    "        token_losses = -token_log_probs  # (batch_size, seq_length-1)\n",
    "        \n",
    "        # Mask padding tokens\n",
    "        token_losses = token_losses * shift_attention  # Zero out losses for padding tokens\n",
    "        print(token_losses)\n",
    "        # Determine the threshold for top 30% losses\n",
    "        # Compute the number of tokens to keep\n",
    "        num_tokens = (shift_attention.sum()).item()\n",
    "        if num_tokens == 0:\n",
    "            continue  # Skip if no tokens to process\n",
    "        k = int(num_tokens * TOP_PERCENT)\n",
    "        if k == 0:\n",
    "            k = 1  # Ensure at least one token is kept\n",
    "        \n",
    "        # Flatten the losses and filter out padding tokens\n",
    "        losses_flat = token_losses.view(-1)\n",
    "        attention_flat = shift_attention.view(-1)\n",
    "        valid_losses = losses_flat[attention_flat == 1]\n",
    "        \n",
    "        if valid_losses.numel() == 0:\n",
    "            continue  # Skip if no valid losses\n",
    "        \n",
    "        # Find the threshold\n",
    "        threshold = torch.topk(valid_losses, k, largest=True, sorted=False).values.min()\n",
    "        \n",
    "        # Create a mask for top 30% losses\n",
    "        mask = (token_losses >= threshold).float()\n",
    "        \n",
    "        # Apply the mask\n",
    "        masked_losses = token_losses * mask\n",
    "        \n",
    "        # Compute the final loss\n",
    "        if mask.sum() == 0:\n",
    "            continue  # Avoid division by zero\n",
    "        final_loss = masked_losses.sum() / mask.sum()\n",
    "        \n",
    "        # Backward pass\n",
    "        final_loss.backward()\n",
    "        epoch_loss += final_loss.item()\n",
    "        \n",
    "        # Gradient accumulation\n",
    "        if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        progress_bar.set_postfix({\"Loss\": final_loss.item()})\n",
    "    \n",
    "    avg_epoch_loss = epoch_loss / len(dataloader)\n",
    "    print(f\"Average Loss for Epoch {epoch + 1}: {avg_epoch_loss}\")\n",
    "\n",
    "# Optionally, save the model\n",
    "# model.save_pretrained(\"llama_finetuned_wikimedia\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
